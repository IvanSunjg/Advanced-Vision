{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path based on your Drive\n",
    "! cd gdrive/MyDrive/AVision_CW\n",
    "! unzip /content/gdrive/MyDrive/AVision_CW/train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are just random not calculated\n",
    "#need to be calculated\n",
    "mean = np.array([0.5, 0.5, 0.5])\n",
    "std = np.array([0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the train data to train and validation sets\n",
    "def train_val_dataset(dataset, val_split=0.25):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    \n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['val'] = Subset(dataset, val_idx)\n",
    "\n",
    "#data augmentation starts\n",
    "#these are random augmentations do not keep them when you use your own augmentation strategy      \n",
    "    datasets['train'].dataset.transform = transforms.Compose([\n",
    "\n",
    "    \ttransforms.RandomResizedCrop(224),\n",
    "    \ttransforms.RandomHorizontalFlip(),\n",
    "    \ttransforms.ToTensor(),\n",
    "    \ttransforms.Normalize(mean, std)\n",
    "            ])\n",
    "            \n",
    "    datasets['val'].dataset.transform = transforms.Compose([\n",
    " \t\t\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "            ])\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads the unzipped train folder from colab\n",
    "dataset = ImageFolder('/content/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the number of total images/train images/val images\n",
    "print(len(dataset))\n",
    "image_datasets = train_val_dataset(dataset)\n",
    "print(len(image_datasets['train']))\n",
    "print(len(image_datasets['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fastest loader on colab with batch_size=64 and num_workers=2\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "              for x in ['train','val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,dataloaders,criterion,scheduler,optimizer,num_epochs=25):\n",
    "\n",
    "  # a dictionary used to store all loss and accuracy\n",
    "  stats = {\"train_loss\":[],\"train_acc\":[],\"val_acc\":[],\"val_loss\":[]}\n",
    "\n",
    "  # training by epoch\n",
    "  for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "    print('-' * 10)\n",
    "    # train\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    t1 = time.perf_counter()\n",
    "    for step, data in enumerate(dataloaders['train'], start=0):\n",
    "      images, labels = data\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images.to(device))\n",
    "      loss = criterion(outputs, labels.to(device))\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      # get the training accuracy\n",
    "      _, predicted = outputs.max(1)\n",
    "      correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "      # print train process\n",
    "      rate = (step + 1) / len(dataloaders['train'])\n",
    "      a = \"*\" * int(rate * 50)\n",
    "      b = \".\" * int((1 - rate) * 50)\n",
    "\n",
    "      print(\"\\rtrain loss: {:^3.0f}%[{}->{}]{:.3f}\".format(int(rate * 100), a, b, loss), end=\"\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # print statistics\n",
    "    print()\n",
    "    print(\"running time is: \" + str(time.perf_counter()-t1) + \" seconds\")\n",
    "\n",
    "    #calculate the training accuracy\n",
    "    train_accurate = correct / len(dataloaders['train'])\n",
    "\n",
    "    # validate\n",
    "    model.eval()  \n",
    "    acc = 0.0  # accumulate accurate number / epoch\n",
    "    best_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for v_step,val_data in enumerate(dataloaders['val']):\n",
    "        val_images, val_labels = val_data\n",
    "        outputs = model(val_images.to(device))\n",
    "        predict_y = torch.max(outputs, dim=1)[1]\n",
    "        acc += (predict_y == val_labels.to(device)).sum().item()\n",
    "        val_loss += criterion(outputs, val_labels.to(device)).item()\n",
    "      # Calculate the validation accuracy\n",
    "      val_accurate = acc / len(dataloaders['val'])\n",
    "      if val_accurate > best_acc:\n",
    "        best_acc = val_accurate\n",
    "\n",
    "    print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' % (epoch + 1, running_loss / step, val_accurate))\n",
    "    \n",
    "    # save the status\n",
    "\n",
    "    stats[\"train_loss\"].append(running_loss / step)\n",
    "    stats[\"train_acc\"].append(train_accurate)\n",
    "    stats[\"val_acc\"].append(val_accurate)\n",
    "    stats[\"val_loss\"].append(val_loss / v_step)\n",
    "\n",
    "\n",
    "  return stats, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# load checkpoint from google drive\n",
    "# FILE = \"/content/gdrive/MyDrive/resnet50_fconv_model_best.pth.tar\"\n",
    "# checkpoint = torch.load(FILE)\n",
    "# model = nn.DataParallel(model)\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# start to train\n",
    "stats, _ = train_model(model, dataloaders, criterion, step_lr_scheduler, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure(name, stats):\n",
    "  fig_1 = plt.figure(figsize=(8, 4))\n",
    "  ax_1 = fig_1.add_subplot(111)\n",
    "  for k in ['train_loss', 'val_loss']:\n",
    "    item = stats[k]\n",
    "    ax_1.plot(np.arange(1, len(item)+1), item, label='{}_{}'.format(name, k))\n",
    "\n",
    "  ax_1.legend(loc=0)\n",
    "  ax_1.set_ylabel('Loss')\n",
    "  ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "  # Plot the change in the validation and training set accuracy over training.\n",
    "  fig_2 = plt.figure(figsize=(8, 4))\n",
    "  ax_2 = fig_2.add_subplot(111)\n",
    "\n",
    "  for k in ['train_acc', 'val_acc']:\n",
    "    item = stats[k]\n",
    "    ax_2.plot(np.arange(1, len(item)+1), item, label='{}_{}'.format(name, k))\n",
    "\n",
    "  ax_2.legend(loc=0)\n",
    "  ax_2.set_ylabel('Accuracy')\n",
    "  ax_2.set_xlabel('Epoch number')\n",
    "\n",
    "  return fig_1, fig_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plot_figure(\"Res50\",stats)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
